# Corpus
Word Embedding benchmark project By Shahid Beheshti University NLP Lab

Please read [Our Wiki Page](https://github.com/sehsanm/embedding-benchmark/wiki) for more information

## Wiki Dump
WikiDump is a complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. A good and public resource for academic researching.
    
This link contains the extracted text from FaWiki XML file.
    
First, we extracted the text data. Next, we normalized it and simply segmented its sentences using regular expressions.
    
You can download the corpus using this [LINK](https://sbuacir-my.sharepoint.com/:f:/g/personal/se_mahmoudi_sbu_ac_ir/EtCWTI-YEoRLiA3G3GtZOPQByjWXef5qPthP-XzIY3xqdA?e=sdbsjY) here

## IrBlogs
irBlogs is a standard Persian weblogs collection that is suitable for studying Persian social networks and evaluation of graph mining and blog retrieval algorithms.

You can find the collection [here](http://dbrg.ut.ac.ir/irblogs/)

## Persian News Corpus
Persian News Corpus contains more than 120 million sentences from tnews.

You can download corpus from [here](https://sbuacir-my.sharepoint.com/:f:/g/personal/se_mahmoudi_sbu_ac_ir/EtCWTI-YEoRLiA3G3GtZOPQByjWXef5qPthP-XzIY3xqdA?e=sdbsjY)
